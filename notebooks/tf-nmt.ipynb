{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of tf-nmt.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDhcZb20ek-g",
        "outputId": "089bece2-2873-4b6d-be01-a468f828d3c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "  !ls -lrt\n",
        "#download data\n",
        "print(\"Downloading Dataset:\")\n",
        "!wget --quiet http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 41828\n",
            "-rw-r--r-- 1 root root  8324970 Aug 23 14:57 deu-eng.zip\n",
            "-rw-r--r-- 1 root root 34494242 Aug 23 23:57 deu.txt\n",
            "-rw-r--r-- 1 root root     1441 Aug 23 23:57 _about.txt\n",
            "drwxr-xr-x 1 root root     4096 Oct  5 16:31 sample_data\n",
            "Downloading Dataset:\n",
            "Archive:  deu-eng.zip\n",
            "replace deu.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: deu.txt                 \n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb5Nyl2Jilu5",
        "outputId": "f68fced6-3521-47cc-9acc-321c460be04c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "ls -lrt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 49960\n",
            "-rw-r--r-- 1 root root  8324970 Aug 23 14:57 deu-eng.zip.1\n",
            "-rw-r--r-- 1 root root  8324970 Aug 23 14:57 deu-eng.zip\n",
            "-rw-r--r-- 1 root root 34494242 Aug 23 23:57 deu.txt\n",
            "-rw-r--r-- 1 root root     1441 Aug 23 23:57 _about.txt\n",
            "drwxr-xr-x 1 root root     4096 Oct  5 16:31 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH5-cYUOergC"
      },
      "source": [
        "import csv\n",
        "import string\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "import numpy as np\n",
        "import itertools\n",
        "from pickle import load\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from pickle import load\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgiBBy92exw8"
      },
      "source": [
        "# Start of sentence\n",
        "SOS = \"<start>\"\n",
        "# End of sentence\n",
        "EOS = \"<end>\"\n",
        "# Relavent Punctuation\n",
        "PUNCTUATION = set(\"?,!,.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI7DkqnpfIGN"
      },
      "source": [
        "def load_dataset(filename: str) -> str:\n",
        "  \"\"\"\n",
        "  load dataset into memory\n",
        "  \"\"\"\n",
        "  with open(filename, mode=\"rt\", encoding=\"utf-8\") as fp:\n",
        "    return fp.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqpy0jfwfooF"
      },
      "source": [
        "def to_pairs(dataset: str, limit: int=None, shuffle=False) -> List[Tuple[str, str]]:\n",
        "  \"\"\"\n",
        "  Split dataset into pair of sentences\n",
        "  :param dataset: dataset containing examples of translations between two languages\n",
        "  the examples are delimited by `\\n` and the contents of the lines are\n",
        "  delimited by `\\t`\n",
        "  :param limit: number that limit dataset size (optional)\n",
        "  :param shuffle: default is True\n",
        "  :return: list of pairs\n",
        "  \"\"\"\n",
        "  assert isinstance(limit, (int, type(None))), TypeError(\n",
        "      \"The limit value must be an integer\"\n",
        "  )\n",
        "  lines = dataset.strip().split('\\n')\n",
        "  # Random Dataset\n",
        "  if shuffle is True:\n",
        "    random.shuffle(lines)\n",
        "  number_examples = limit or len(lines)\n",
        "  pairs = []\n",
        "  for line in lines[: abs(number_examples)]:\n",
        "    # take only source and target\n",
        "    src, trg, _ = line.split(\"\\t\")\n",
        "    pairs.append((src, trg))\n",
        "\n",
        "  # dataset size check\n",
        "  assert len(pairs) == number_examples\n",
        "  return pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSJ-HZIVji4f"
      },
      "source": [
        "def separe_punctuation(token: str) -> str:\n",
        "  \"\"\"\n",
        "  Separe punctuation if they exist\n",
        "  \"\"\"\n",
        "  if not set(token).intersection(PUNCTUATION):\n",
        "    return token\n",
        "  for p in PUNCTUATION:\n",
        "    token = f\" {p} \".join(token.split(p))\n",
        "  return \" \".join(token.split(p))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WC5FstwGKhp"
      },
      "source": [
        "def preprocess(sentence: str, add_start_end: bool=True) -> str:\n",
        "  \"\"\"\n",
        "  - convert lowercase\n",
        "  - remove numbers\n",
        "  - remove speacial characters\n",
        "  - separe punctuation\n",
        "  - add <start> and <end> of sentence\n",
        "  :param add_start_end: add SOS and EOS\n",
        "  \"\"\"\n",
        "  re_print = re.compile(f\"[^{re.escape(string.printable)}]\")\n",
        "  # convert lowercase and normalizing unicode characters\n",
        "  sentence = (\n",
        "      normalize(\"NFD\", sentence.lower()).encode(\"ascii\", \"ignore\").decode(\"UTF-8\")\n",
        "  )\n",
        "  cleaned_tokens = []\n",
        "  # tokenize sentence on white space\n",
        "  for token in sentence.split():\n",
        "    # remove non printable characters from each token\n",
        "    token = re_print.sub(\"\", token).strip()\n",
        "    # ignore token with numbers\n",
        "    if re.findall(\"[0-9]\", token):\n",
        "      continue\n",
        "    # add space between words and punctuation\n",
        "    token = separe_punctuation(token)\n",
        "    cleaned_tokens.append(token)\n",
        "\n",
        "  # rebuild sentence with space between tokens\n",
        "  sentence = \" \".join(cleaned_tokens)\n",
        "\n",
        "  # add a start and end token to the sentence\n",
        "  if add_start_end is True:\n",
        "    sentence = f\"{SOS} {sentence} {EOS}\"\n",
        "  return sentence      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVyQLrfNR7iC"
      },
      "source": [
        "def dataset_preprocess(dataset: List[Tuple[str, str]]) -> Tuple[List[str], List[str]]:\n",
        "  \"\"\"\n",
        "  Returns processed dataset\n",
        "\n",
        "  :param dataset: list of sentence pairs\n",
        "  :return: list of prallel data\n",
        "  \"\"\"\n",
        "  source_cleaned = []\n",
        "  target_cleaned = []\n",
        "  for source, target in dataset:\n",
        "    source_cleaned.append(preprocess(source))\n",
        "    target_cleaned.append(preprocess(target))\n",
        "  return source_cleaned, target_cleaned  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwKm5lYTiHMH",
        "outputId": "ed4f331e-ba76-4141-f419-0667db442d76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "NUM_EXAMPLES = 10000 # Limit dataset size\n",
        "filename = \"deu.txt\"\n",
        "dataset = load_dataset(filename)\n",
        "# get pairs limited to 1000 pairs\n",
        "pairs = to_pairs(dataset, limit=NUM_EXAMPLES)\n",
        "print(f\"Dataset size: {len(pairs)}\")\n",
        "raw_data_en, raw_data_ge = dataset_preprocess(pairs)\n",
        "\n",
        "# show last 5 pairs\n",
        "for pair in zip(raw_data_en[-5:],raw_data_ge[-5:]):\n",
        "    print(pair)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size: 10000\n",
            "('<start> tom was crying    <end>', '<start> tom flennte    <end>')\n",
            "('<start> tom was eating    <end>', '<start> tom hat gegessen    <end>')\n",
            "('<start> tom was famous    <end>', '<start> tom war beruhmt    <end>')\n",
            "('<start> tom was framed    <end>', '<start> tom wurde reingelegt    <end>')\n",
            "('<start> tom was fuming    <end>', '<start> tom war wutend    <end>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pafn9rsGkTyK"
      },
      "source": [
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(raw_data_en)\n",
        "\n",
        "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')\n",
        "\n",
        "ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "ge_tokenizer.fit_on_texts(raw_data_ge)\n",
        "\n",
        "data_ge = ge_tokenizer.texts_to_sequences(raw_data_ge)\n",
        "data_ge = tf.keras.preprocessing.sequence.pad_sequences(data_ge, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQPKXRUopSgD"
      },
      "source": [
        "def max_len(tensor):\n",
        "    #print( np.argmax([len(t) for t in tensor]))\n",
        "    return max( len(t) for t in tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfBasZugI9LF"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(data_en, data_ge, test_size=0.2)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(X_train)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "rnn_units = 1024\n",
        "dense_units = 1024\n",
        "Dtype = tf.float32 #used to initialize DecoderCell Zero State"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiHCxmLGKGcr",
        "outputId": "2d1fc882-6cdb-496a-868d-4e24524b979f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "Tx = max_len(data_en)\n",
        "Ty = max_len(data_ge)\n",
        "\n",
        "input_vocab_size = len(en_tokenizer.word_index)+1\n",
        "output_vocab_size = len(ge_tokenizer.word_index)+ 1\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape)\n",
        "print(example_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 8)\n",
            "(64, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUvFYhsugNuM"
      },
      "source": [
        "# ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "  def __init__(self, input_vocab_size, embedding_dims, rnn_units):\n",
        "    super().__init__()\n",
        "    self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size, output_dim=embedding_dims)\n",
        "    self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "\n",
        "\n",
        "# DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "  def __init__(self, output_vocab_size, embedding_dims, rnn_units):\n",
        "    super().__init__()\n",
        "    self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size, output_dim=embedding_dims)\n",
        "    self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "    self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "    # Sampler\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "    # Create attention mechanism with memory = None\n",
        "    self.attention_mechanism = self.build_attention_mechanism(dense_units, None, BATCH_SIZE*[Tx])\n",
        "    self.rnn_cell = self.build_rnn_cell(BATCH_SIZE)\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.dense_layer)\n",
        "\n",
        "  def build_attention_mechanism(self, units, memory, memory_sequence_length):\n",
        "    return tfa.seq2seq.LuongAttention(units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "    # tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "\n",
        "    # wrap decoderrnn cell\n",
        "  def build_rnn_cell(self, batch_size):\n",
        "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism, attention_layer_size=dense_units)\n",
        "    return rnn_cell\n",
        "\n",
        "  def build_decoder_initial_state(self, batch_size, encoder_state, Dtype):\n",
        "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_size, dtype=Dtype)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "encoderNetwork = EncoderNetwork(input_vocab_size, embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size, embedding_dims, rnn_units)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsvScW4hCGAC"
      },
      "source": [
        "def loss_function(y_pred, y):\n",
        "   \n",
        "    #shape of y [batch_size, ty]\n",
        "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =encoder_initial_cell_state)\n",
        "\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        \n",
        "         \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=[a_tx, c_tx],\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6UnVQP0CK1W"
      },
      "source": [
        "#RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "  return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px7z1yNICM0F",
        "outputId": "5cc88804-d0b5-44f4-ab32-f4a4f6ca90cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 15\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%5 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 4.120637893676758 epoch 1 batch 5 \n",
            "total loss: 2.7828941345214844 epoch 1 batch 10 \n",
            "total loss: 2.3763608932495117 epoch 1 batch 15 \n",
            "total loss: 2.2228238582611084 epoch 1 batch 20 \n",
            "total loss: 2.1833066940307617 epoch 1 batch 25 \n",
            "total loss: 2.0061843395233154 epoch 1 batch 30 \n",
            "total loss: 1.9310638904571533 epoch 1 batch 35 \n",
            "total loss: 1.987554669380188 epoch 1 batch 40 \n",
            "total loss: 1.8907928466796875 epoch 1 batch 45 \n",
            "total loss: 1.8459843397140503 epoch 1 batch 50 \n",
            "total loss: 1.90363347530365 epoch 1 batch 55 \n",
            "total loss: 1.8472965955734253 epoch 1 batch 60 \n",
            "total loss: 2.021408796310425 epoch 1 batch 65 \n",
            "total loss: 1.8862862586975098 epoch 1 batch 70 \n",
            "total loss: 1.761630654335022 epoch 1 batch 75 \n",
            "total loss: 1.7737637758255005 epoch 1 batch 80 \n",
            "total loss: 1.7739166021347046 epoch 1 batch 85 \n",
            "total loss: 1.609162449836731 epoch 1 batch 90 \n",
            "total loss: 1.7197016477584839 epoch 1 batch 95 \n",
            "total loss: 1.71544349193573 epoch 1 batch 100 \n",
            "total loss: 1.7139511108398438 epoch 1 batch 105 \n",
            "total loss: 1.6955739259719849 epoch 1 batch 110 \n",
            "total loss: 1.5636940002441406 epoch 1 batch 115 \n",
            "total loss: 1.568941593170166 epoch 1 batch 120 \n",
            "total loss: 1.463666558265686 epoch 1 batch 125 \n",
            "total loss: 1.584228515625 epoch 2 batch 5 \n",
            "total loss: 1.3826152086257935 epoch 2 batch 10 \n",
            "total loss: 1.6568149328231812 epoch 2 batch 15 \n",
            "total loss: 1.6618313789367676 epoch 2 batch 20 \n",
            "total loss: 1.5755488872528076 epoch 2 batch 25 \n",
            "total loss: 1.586959719657898 epoch 2 batch 30 \n",
            "total loss: 1.4503960609436035 epoch 2 batch 35 \n",
            "total loss: 1.5615168809890747 epoch 2 batch 40 \n",
            "total loss: 1.3583112955093384 epoch 2 batch 45 \n",
            "total loss: 1.5837076902389526 epoch 2 batch 50 \n",
            "total loss: 1.4416747093200684 epoch 2 batch 55 \n",
            "total loss: 1.4090237617492676 epoch 2 batch 60 \n",
            "total loss: 1.4088356494903564 epoch 2 batch 65 \n",
            "total loss: 1.423831582069397 epoch 2 batch 70 \n",
            "total loss: 1.3723771572113037 epoch 2 batch 75 \n",
            "total loss: 1.381913661956787 epoch 2 batch 80 \n",
            "total loss: 1.3047457933425903 epoch 2 batch 85 \n",
            "total loss: 1.2976588010787964 epoch 2 batch 90 \n",
            "total loss: 1.3927384614944458 epoch 2 batch 95 \n",
            "total loss: 1.4048880338668823 epoch 2 batch 100 \n",
            "total loss: 1.4150127172470093 epoch 2 batch 105 \n",
            "total loss: 1.280776858329773 epoch 2 batch 110 \n",
            "total loss: 1.3573917150497437 epoch 2 batch 115 \n",
            "total loss: 1.3378770351409912 epoch 2 batch 120 \n",
            "total loss: 1.2372866868972778 epoch 2 batch 125 \n",
            "total loss: 1.1984034776687622 epoch 3 batch 5 \n",
            "total loss: 1.2111117839813232 epoch 3 batch 10 \n",
            "total loss: 1.2515761852264404 epoch 3 batch 15 \n",
            "total loss: 1.2208775281906128 epoch 3 batch 20 \n",
            "total loss: 1.2919301986694336 epoch 3 batch 25 \n",
            "total loss: 1.097662329673767 epoch 3 batch 30 \n",
            "total loss: 1.159043312072754 epoch 3 batch 35 \n",
            "total loss: 1.1924700736999512 epoch 3 batch 40 \n",
            "total loss: 1.2314645051956177 epoch 3 batch 45 \n",
            "total loss: 1.2958136796951294 epoch 3 batch 50 \n",
            "total loss: 1.2774949073791504 epoch 3 batch 55 \n",
            "total loss: 1.184399127960205 epoch 3 batch 60 \n",
            "total loss: 1.2897202968597412 epoch 3 batch 65 \n",
            "total loss: 1.3793565034866333 epoch 3 batch 70 \n",
            "total loss: 1.2396382093429565 epoch 3 batch 75 \n",
            "total loss: 1.1419150829315186 epoch 3 batch 80 \n",
            "total loss: 1.1692090034484863 epoch 3 batch 85 \n",
            "total loss: 1.2386616468429565 epoch 3 batch 90 \n",
            "total loss: 1.167922854423523 epoch 3 batch 95 \n",
            "total loss: 1.2018370628356934 epoch 3 batch 100 \n",
            "total loss: 1.25422203540802 epoch 3 batch 105 \n",
            "total loss: 1.1099892854690552 epoch 3 batch 110 \n",
            "total loss: 1.2404526472091675 epoch 3 batch 115 \n",
            "total loss: 1.1803040504455566 epoch 3 batch 120 \n",
            "total loss: 1.242008924484253 epoch 3 batch 125 \n",
            "total loss: 1.0776379108428955 epoch 4 batch 5 \n",
            "total loss: 1.1560072898864746 epoch 4 batch 10 \n",
            "total loss: 1.0450493097305298 epoch 4 batch 15 \n",
            "total loss: 1.146531105041504 epoch 4 batch 20 \n",
            "total loss: 0.9759089946746826 epoch 4 batch 25 \n",
            "total loss: 1.072158932685852 epoch 4 batch 30 \n",
            "total loss: 1.061147689819336 epoch 4 batch 35 \n",
            "total loss: 1.1363743543624878 epoch 4 batch 40 \n",
            "total loss: 1.0069832801818848 epoch 4 batch 45 \n",
            "total loss: 1.034196138381958 epoch 4 batch 50 \n",
            "total loss: 0.9668941497802734 epoch 4 batch 55 \n",
            "total loss: 1.137644648551941 epoch 4 batch 60 \n",
            "total loss: 0.9185574054718018 epoch 4 batch 65 \n",
            "total loss: 0.9878667593002319 epoch 4 batch 70 \n",
            "total loss: 1.0323493480682373 epoch 4 batch 75 \n",
            "total loss: 0.9829063415527344 epoch 4 batch 80 \n",
            "total loss: 1.1019467115402222 epoch 4 batch 85 \n",
            "total loss: 1.0313910245895386 epoch 4 batch 90 \n",
            "total loss: 0.9854760766029358 epoch 4 batch 95 \n",
            "total loss: 0.9766948819160461 epoch 4 batch 100 \n",
            "total loss: 0.9926082491874695 epoch 4 batch 105 \n",
            "total loss: 0.9495261907577515 epoch 4 batch 110 \n",
            "total loss: 1.0706063508987427 epoch 4 batch 115 \n",
            "total loss: 1.047537088394165 epoch 4 batch 120 \n",
            "total loss: 1.0283825397491455 epoch 4 batch 125 \n",
            "total loss: 0.9020223021507263 epoch 5 batch 5 \n",
            "total loss: 0.8336513638496399 epoch 5 batch 10 \n",
            "total loss: 0.8581626415252686 epoch 5 batch 15 \n",
            "total loss: 0.842266321182251 epoch 5 batch 20 \n",
            "total loss: 0.729661226272583 epoch 5 batch 25 \n",
            "total loss: 0.8760101795196533 epoch 5 batch 30 \n",
            "total loss: 0.9724903702735901 epoch 5 batch 35 \n",
            "total loss: 0.8144540190696716 epoch 5 batch 40 \n",
            "total loss: 0.8399286866188049 epoch 5 batch 45 \n",
            "total loss: 0.7901748418807983 epoch 5 batch 50 \n",
            "total loss: 0.833333432674408 epoch 5 batch 55 \n",
            "total loss: 0.8647884726524353 epoch 5 batch 60 \n",
            "total loss: 0.8337160348892212 epoch 5 batch 65 \n",
            "total loss: 0.7689256072044373 epoch 5 batch 70 \n",
            "total loss: 1.0023443698883057 epoch 5 batch 75 \n",
            "total loss: 0.8671960830688477 epoch 5 batch 80 \n",
            "total loss: 0.9467136263847351 epoch 5 batch 85 \n",
            "total loss: 0.8536028265953064 epoch 5 batch 90 \n",
            "total loss: 0.9173497557640076 epoch 5 batch 95 \n",
            "total loss: 0.9339483380317688 epoch 5 batch 100 \n",
            "total loss: 0.9747726917266846 epoch 5 batch 105 \n",
            "total loss: 0.817633867263794 epoch 5 batch 110 \n",
            "total loss: 0.8890426158905029 epoch 5 batch 115 \n",
            "total loss: 0.7954976558685303 epoch 5 batch 120 \n",
            "total loss: 0.8943307995796204 epoch 5 batch 125 \n",
            "total loss: 0.6653420925140381 epoch 6 batch 5 \n",
            "total loss: 0.7073791027069092 epoch 6 batch 10 \n",
            "total loss: 0.7031631469726562 epoch 6 batch 15 \n",
            "total loss: 0.7312760949134827 epoch 6 batch 20 \n",
            "total loss: 0.6775771379470825 epoch 6 batch 25 \n",
            "total loss: 0.6812251806259155 epoch 6 batch 30 \n",
            "total loss: 0.6792711019515991 epoch 6 batch 35 \n",
            "total loss: 0.7221351265907288 epoch 6 batch 40 \n",
            "total loss: 0.593389093875885 epoch 6 batch 45 \n",
            "total loss: 0.7321134209632874 epoch 6 batch 50 \n",
            "total loss: 0.7266347408294678 epoch 6 batch 55 \n",
            "total loss: 0.7270267009735107 epoch 6 batch 60 \n",
            "total loss: 0.6096380949020386 epoch 6 batch 65 \n",
            "total loss: 0.7413106560707092 epoch 6 batch 70 \n",
            "total loss: 0.7428245544433594 epoch 6 batch 75 \n",
            "total loss: 0.6532874703407288 epoch 6 batch 80 \n",
            "total loss: 0.7676767706871033 epoch 6 batch 85 \n",
            "total loss: 0.7674286365509033 epoch 6 batch 90 \n",
            "total loss: 0.6565309166908264 epoch 6 batch 95 \n",
            "total loss: 0.6611349582672119 epoch 6 batch 100 \n",
            "total loss: 0.7815937399864197 epoch 6 batch 105 \n",
            "total loss: 0.6545127630233765 epoch 6 batch 110 \n",
            "total loss: 0.7293066382408142 epoch 6 batch 115 \n",
            "total loss: 0.7239461541175842 epoch 6 batch 120 \n",
            "total loss: 0.7980308532714844 epoch 6 batch 125 \n",
            "total loss: 0.5301053524017334 epoch 7 batch 5 \n",
            "total loss: 0.47828608751296997 epoch 7 batch 10 \n",
            "total loss: 0.49983900785446167 epoch 7 batch 15 \n",
            "total loss: 0.5016518235206604 epoch 7 batch 20 \n",
            "total loss: 0.5828933715820312 epoch 7 batch 25 \n",
            "total loss: 0.5583462119102478 epoch 7 batch 30 \n",
            "total loss: 0.513981819152832 epoch 7 batch 35 \n",
            "total loss: 0.6436060070991516 epoch 7 batch 40 \n",
            "total loss: 0.5864747762680054 epoch 7 batch 45 \n",
            "total loss: 0.49277088046073914 epoch 7 batch 50 \n",
            "total loss: 0.6450564861297607 epoch 7 batch 55 \n",
            "total loss: 0.5630638599395752 epoch 7 batch 60 \n",
            "total loss: 0.6164731383323669 epoch 7 batch 65 \n",
            "total loss: 0.5284703969955444 epoch 7 batch 70 \n",
            "total loss: 0.5026307702064514 epoch 7 batch 75 \n",
            "total loss: 0.5987724661827087 epoch 7 batch 80 \n",
            "total loss: 0.6102483868598938 epoch 7 batch 85 \n",
            "total loss: 0.6150354743003845 epoch 7 batch 90 \n",
            "total loss: 0.5704507231712341 epoch 7 batch 95 \n",
            "total loss: 0.7464403510093689 epoch 7 batch 100 \n",
            "total loss: 0.6188137531280518 epoch 7 batch 105 \n",
            "total loss: 0.6552355885505676 epoch 7 batch 110 \n",
            "total loss: 0.49117374420166016 epoch 7 batch 115 \n",
            "total loss: 0.7138515710830688 epoch 7 batch 120 \n",
            "total loss: 0.5134516358375549 epoch 7 batch 125 \n",
            "total loss: 0.37921878695487976 epoch 8 batch 5 \n",
            "total loss: 0.3765409588813782 epoch 8 batch 10 \n",
            "total loss: 0.40688055753707886 epoch 8 batch 15 \n",
            "total loss: 0.34907767176628113 epoch 8 batch 20 \n",
            "total loss: 0.3984068036079407 epoch 8 batch 25 \n",
            "total loss: 0.4850493371486664 epoch 8 batch 30 \n",
            "total loss: 0.4478450119495392 epoch 8 batch 35 \n",
            "total loss: 0.47066688537597656 epoch 8 batch 40 \n",
            "total loss: 0.4908387064933777 epoch 8 batch 45 \n",
            "total loss: 0.4363527297973633 epoch 8 batch 50 \n",
            "total loss: 0.4573310613632202 epoch 8 batch 55 \n",
            "total loss: 0.46826010942459106 epoch 8 batch 60 \n",
            "total loss: 0.49832457304000854 epoch 8 batch 65 \n",
            "total loss: 0.4568948745727539 epoch 8 batch 70 \n",
            "total loss: 0.5006973743438721 epoch 8 batch 75 \n",
            "total loss: 0.5036278963088989 epoch 8 batch 80 \n",
            "total loss: 0.5002333521842957 epoch 8 batch 85 \n",
            "total loss: 0.43103790283203125 epoch 8 batch 90 \n",
            "total loss: 0.46414729952812195 epoch 8 batch 95 \n",
            "total loss: 0.5093573927879333 epoch 8 batch 100 \n",
            "total loss: 0.42589616775512695 epoch 8 batch 105 \n",
            "total loss: 0.40135496854782104 epoch 8 batch 110 \n",
            "total loss: 0.44852229952812195 epoch 8 batch 115 \n",
            "total loss: 0.471871942281723 epoch 8 batch 120 \n",
            "total loss: 0.5082932114601135 epoch 8 batch 125 \n",
            "total loss: 0.32429540157318115 epoch 9 batch 5 \n",
            "total loss: 0.2533849775791168 epoch 9 batch 10 \n",
            "total loss: 0.41520434617996216 epoch 9 batch 15 \n",
            "total loss: 0.26696309447288513 epoch 9 batch 20 \n",
            "total loss: 0.39038172364234924 epoch 9 batch 25 \n",
            "total loss: 0.3242388963699341 epoch 9 batch 30 \n",
            "total loss: 0.36235544085502625 epoch 9 batch 35 \n",
            "total loss: 0.31577932834625244 epoch 9 batch 40 \n",
            "total loss: 0.3862254321575165 epoch 9 batch 45 \n",
            "total loss: 0.3069165050983429 epoch 9 batch 50 \n",
            "total loss: 0.34582188725471497 epoch 9 batch 55 \n",
            "total loss: 0.3250213861465454 epoch 9 batch 60 \n",
            "total loss: 0.3886629045009613 epoch 9 batch 65 \n",
            "total loss: 0.3829082250595093 epoch 9 batch 70 \n",
            "total loss: 0.38939839601516724 epoch 9 batch 75 \n",
            "total loss: 0.42898988723754883 epoch 9 batch 80 \n",
            "total loss: 0.3936140835285187 epoch 9 batch 85 \n",
            "total loss: 0.3585872948169708 epoch 9 batch 90 \n",
            "total loss: 0.39402052760124207 epoch 9 batch 95 \n",
            "total loss: 0.3970208466053009 epoch 9 batch 100 \n",
            "total loss: 0.3520148992538452 epoch 9 batch 105 \n",
            "total loss: 0.40113431215286255 epoch 9 batch 110 \n",
            "total loss: 0.47405025362968445 epoch 9 batch 115 \n",
            "total loss: 0.430245041847229 epoch 9 batch 120 \n",
            "total loss: 0.40684136748313904 epoch 9 batch 125 \n",
            "total loss: 0.25652945041656494 epoch 10 batch 5 \n",
            "total loss: 0.3121279180049896 epoch 10 batch 10 \n",
            "total loss: 0.27881890535354614 epoch 10 batch 15 \n",
            "total loss: 0.255563348531723 epoch 10 batch 20 \n",
            "total loss: 0.29590389132499695 epoch 10 batch 25 \n",
            "total loss: 0.34870195388793945 epoch 10 batch 30 \n",
            "total loss: 0.31283363699913025 epoch 10 batch 35 \n",
            "total loss: 0.33097317814826965 epoch 10 batch 40 \n",
            "total loss: 0.2988634407520294 epoch 10 batch 45 \n",
            "total loss: 0.3085940182209015 epoch 10 batch 50 \n",
            "total loss: 0.29732850193977356 epoch 10 batch 55 \n",
            "total loss: 0.30190953612327576 epoch 10 batch 60 \n",
            "total loss: 0.3038496673107147 epoch 10 batch 65 \n",
            "total loss: 0.2857016623020172 epoch 10 batch 70 \n",
            "total loss: 0.395945280790329 epoch 10 batch 75 \n",
            "total loss: 0.3150499165058136 epoch 10 batch 80 \n",
            "total loss: 0.34328263998031616 epoch 10 batch 85 \n",
            "total loss: 0.3979976773262024 epoch 10 batch 90 \n",
            "total loss: 0.2520946264266968 epoch 10 batch 95 \n",
            "total loss: 0.3981020748615265 epoch 10 batch 100 \n",
            "total loss: 0.2913846969604492 epoch 10 batch 105 \n",
            "total loss: 0.33444300293922424 epoch 10 batch 110 \n",
            "total loss: 0.3423417806625366 epoch 10 batch 115 \n",
            "total loss: 0.33820897340774536 epoch 10 batch 120 \n",
            "total loss: 0.31384822726249695 epoch 10 batch 125 \n",
            "total loss: 0.2599951922893524 epoch 11 batch 5 \n",
            "total loss: 0.23120523989200592 epoch 11 batch 10 \n",
            "total loss: 0.24113135039806366 epoch 11 batch 15 \n",
            "total loss: 0.2773658037185669 epoch 11 batch 20 \n",
            "total loss: 0.21394182741641998 epoch 11 batch 25 \n",
            "total loss: 0.2512187063694 epoch 11 batch 30 \n",
            "total loss: 0.22506462037563324 epoch 11 batch 35 \n",
            "total loss: 0.25075212121009827 epoch 11 batch 40 \n",
            "total loss: 0.22673483192920685 epoch 11 batch 45 \n",
            "total loss: 0.2909015715122223 epoch 11 batch 50 \n",
            "total loss: 0.2416406273841858 epoch 11 batch 55 \n",
            "total loss: 0.23033028841018677 epoch 11 batch 60 \n",
            "total loss: 0.23299670219421387 epoch 11 batch 65 \n",
            "total loss: 0.27577948570251465 epoch 11 batch 70 \n",
            "total loss: 0.27279186248779297 epoch 11 batch 75 \n",
            "total loss: 0.2873649299144745 epoch 11 batch 80 \n",
            "total loss: 0.28454917669296265 epoch 11 batch 85 \n",
            "total loss: 0.28546467423439026 epoch 11 batch 90 \n",
            "total loss: 0.31163427233695984 epoch 11 batch 95 \n",
            "total loss: 0.32626867294311523 epoch 11 batch 100 \n",
            "total loss: 0.2760443687438965 epoch 11 batch 105 \n",
            "total loss: 0.30208685994148254 epoch 11 batch 110 \n",
            "total loss: 0.33530962467193604 epoch 11 batch 115 \n",
            "total loss: 0.30423814058303833 epoch 11 batch 120 \n",
            "total loss: 0.32664990425109863 epoch 11 batch 125 \n",
            "total loss: 0.21391282975673676 epoch 12 batch 5 \n",
            "total loss: 0.21153631806373596 epoch 12 batch 10 \n",
            "total loss: 0.2370682656764984 epoch 12 batch 15 \n",
            "total loss: 0.20224781334400177 epoch 12 batch 20 \n",
            "total loss: 0.22296376526355743 epoch 12 batch 25 \n",
            "total loss: 0.2177506387233734 epoch 12 batch 30 \n",
            "total loss: 0.21959976851940155 epoch 12 batch 35 \n",
            "total loss: 0.2123323380947113 epoch 12 batch 40 \n",
            "total loss: 0.20424693822860718 epoch 12 batch 45 \n",
            "total loss: 0.19867068529129028 epoch 12 batch 50 \n",
            "total loss: 0.25051891803741455 epoch 12 batch 55 \n",
            "total loss: 0.23853133618831635 epoch 12 batch 60 \n",
            "total loss: 0.27978619933128357 epoch 12 batch 65 \n",
            "total loss: 0.2793923318386078 epoch 12 batch 70 \n",
            "total loss: 0.21704627573490143 epoch 12 batch 75 \n",
            "total loss: 0.24072621762752533 epoch 12 batch 80 \n",
            "total loss: 0.2532833516597748 epoch 12 batch 85 \n",
            "total loss: 0.24450641870498657 epoch 12 batch 90 \n",
            "total loss: 0.2852639853954315 epoch 12 batch 95 \n",
            "total loss: 0.2550031542778015 epoch 12 batch 100 \n",
            "total loss: 0.22904634475708008 epoch 12 batch 105 \n",
            "total loss: 0.288941890001297 epoch 12 batch 110 \n",
            "total loss: 0.29577142000198364 epoch 12 batch 115 \n",
            "total loss: 0.26289770007133484 epoch 12 batch 120 \n",
            "total loss: 0.25611451268196106 epoch 12 batch 125 \n",
            "total loss: 0.1564350426197052 epoch 13 batch 5 \n",
            "total loss: 0.15642741322517395 epoch 13 batch 10 \n",
            "total loss: 0.21488219499588013 epoch 13 batch 15 \n",
            "total loss: 0.2231454849243164 epoch 13 batch 20 \n",
            "total loss: 0.21271581947803497 epoch 13 batch 25 \n",
            "total loss: 0.19019603729248047 epoch 13 batch 30 \n",
            "total loss: 0.18968243896961212 epoch 13 batch 35 \n",
            "total loss: 0.19028347730636597 epoch 13 batch 40 \n",
            "total loss: 0.20068226754665375 epoch 13 batch 45 \n",
            "total loss: 0.19580420851707458 epoch 13 batch 50 \n",
            "total loss: 0.18769951164722443 epoch 13 batch 55 \n",
            "total loss: 0.19499821960926056 epoch 13 batch 60 \n",
            "total loss: 0.2275819331407547 epoch 13 batch 65 \n",
            "total loss: 0.3058558702468872 epoch 13 batch 70 \n",
            "total loss: 0.2611190974712372 epoch 13 batch 75 \n",
            "total loss: 0.2512308359146118 epoch 13 batch 80 \n",
            "total loss: 0.18018531799316406 epoch 13 batch 85 \n",
            "total loss: 0.20880970358848572 epoch 13 batch 90 \n",
            "total loss: 0.18899302184581757 epoch 13 batch 95 \n",
            "total loss: 0.25084832310676575 epoch 13 batch 100 \n",
            "total loss: 0.27125290036201477 epoch 13 batch 105 \n",
            "total loss: 0.23125340044498444 epoch 13 batch 110 \n",
            "total loss: 0.2136932760477066 epoch 13 batch 115 \n",
            "total loss: 0.2490212768316269 epoch 13 batch 120 \n",
            "total loss: 0.25120365619659424 epoch 13 batch 125 \n",
            "total loss: 0.14619246125221252 epoch 14 batch 5 \n",
            "total loss: 0.14908482134342194 epoch 14 batch 10 \n",
            "total loss: 0.1516653448343277 epoch 14 batch 15 \n",
            "total loss: 0.1579449623823166 epoch 14 batch 20 \n",
            "total loss: 0.1406961977481842 epoch 14 batch 25 \n",
            "total loss: 0.13309146463871002 epoch 14 batch 30 \n",
            "total loss: 0.19817329943180084 epoch 14 batch 35 \n",
            "total loss: 0.15973737835884094 epoch 14 batch 40 \n",
            "total loss: 0.15819622576236725 epoch 14 batch 45 \n",
            "total loss: 0.20505014061927795 epoch 14 batch 50 \n",
            "total loss: 0.19885310530662537 epoch 14 batch 55 \n",
            "total loss: 0.22335438430309296 epoch 14 batch 60 \n",
            "total loss: 0.1792890876531601 epoch 14 batch 65 \n",
            "total loss: 0.18809013068675995 epoch 14 batch 70 \n",
            "total loss: 0.21838916838169098 epoch 14 batch 75 \n",
            "total loss: 0.21556471288204193 epoch 14 batch 80 \n",
            "total loss: 0.19461575150489807 epoch 14 batch 85 \n",
            "total loss: 0.20015473663806915 epoch 14 batch 90 \n",
            "total loss: 0.16033412516117096 epoch 14 batch 95 \n",
            "total loss: 0.2199939340353012 epoch 14 batch 100 \n",
            "total loss: 0.1797696053981781 epoch 14 batch 105 \n",
            "total loss: 0.16381818056106567 epoch 14 batch 110 \n",
            "total loss: 0.15930287539958954 epoch 14 batch 115 \n",
            "total loss: 0.2462172955274582 epoch 14 batch 120 \n",
            "total loss: 0.21695728600025177 epoch 14 batch 125 \n",
            "total loss: 0.1504824012517929 epoch 15 batch 5 \n",
            "total loss: 0.15799027681350708 epoch 15 batch 10 \n",
            "total loss: 0.16338232159614563 epoch 15 batch 15 \n",
            "total loss: 0.1379285752773285 epoch 15 batch 20 \n",
            "total loss: 0.10951992869377136 epoch 15 batch 25 \n",
            "total loss: 0.13913308084011078 epoch 15 batch 30 \n",
            "total loss: 0.13572946190834045 epoch 15 batch 35 \n",
            "total loss: 0.15403494238853455 epoch 15 batch 40 \n",
            "total loss: 0.16797083616256714 epoch 15 batch 45 \n",
            "total loss: 0.18057650327682495 epoch 15 batch 50 \n",
            "total loss: 0.1393079310655594 epoch 15 batch 55 \n",
            "total loss: 0.1915934532880783 epoch 15 batch 60 \n",
            "total loss: 0.1417086124420166 epoch 15 batch 65 \n",
            "total loss: 0.13050717115402222 epoch 15 batch 70 \n",
            "total loss: 0.2154776006937027 epoch 15 batch 75 \n",
            "total loss: 0.18476420640945435 epoch 15 batch 80 \n",
            "total loss: 0.1949159950017929 epoch 15 batch 85 \n",
            "total loss: 0.19477657973766327 epoch 15 batch 90 \n",
            "total loss: 0.19582681357860565 epoch 15 batch 95 \n",
            "total loss: 0.17080450057983398 epoch 15 batch 100 \n",
            "total loss: 0.19853629171848297 epoch 15 batch 105 \n",
            "total loss: 0.21161361038684845 epoch 15 batch 110 \n",
            "total loss: 0.1748584657907486 epoch 15 batch 115 \n",
            "total loss: 0.21959061920642853 epoch 15 batch 120 \n",
            "total loss: 0.22398242354393005 epoch 15 batch 125 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA16mm0FCR9O",
        "outputId": "ac18168b-aa1b-4e04-f5e1-3761bea4c16a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "#In this section we evaluate our model on a raw_input converted to german, for this the entire sentence has to be passed\n",
        "#through the length of the model, for this we use greedsampler to run through the decoder\n",
        "#and the final embedding matrix trained on the data is used to generate embeddings\n",
        "input_raw='how are you'\n",
        "\n",
        "# We have a transcript file containing English-German pairs\n",
        "# Preprocess X\n",
        "input_raw = preprocess(input_raw, add_start_end=False)\n",
        "input_lines = [f'{SOS} {input_raw}']\n",
        "input_sequences = [[en_tokenizer.word_index[w] for w in line.split()] for line in input_lines]\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                maxlen=Tx, padding='post')\n",
        "inp = tf.convert_to_tensor(input_sequences)\n",
        "#print(inp.shape)\n",
        "inference_batch_size = input_sequences.shape[0]\n",
        "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                              tf.zeros((inference_batch_size, rnn_units))]\n",
        "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                initial_state =encoder_initial_cell_state)\n",
        "print('a_tx :', a_tx.shape)\n",
        "print('c_tx :', c_tx.shape)\n",
        "\n",
        "start_tokens = tf.fill([inference_batch_size],ge_tokenizer.word_index[SOS])\n",
        "\n",
        "end_token = ge_tokenizer.word_index[EOS]\n",
        "\n",
        "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "decoder_input = tf.expand_dims([ge_tokenizer.word_index[SOS]]* inference_batch_size,1)\n",
        "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
        "                                            output_layer=decoderNetwork.dense_layer)\n",
        "decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "print(f\"decoder_initial_state = [a_tx, c_tx] : {np.array([a_tx, c_tx]).shape}\")\n",
        "decoder_initial_state = decoderNetwork.build_decoder_initial_state(inference_batch_size,\n",
        "                                                                   encoder_state=[a_tx, c_tx],\n",
        "                                                                   Dtype=tf.float32)\n",
        "print(f\"\"\"\n",
        "Compared to simple encoder-decoder without attention, the decoder_initial_state\n",
        "is an AttentionWrapperState object containing s_prev tensors and context and alignment vector\n",
        "\n",
        "decoder initial state shape: {np.array(decoder_initial_state).shape}\n",
        "decoder_initial_state tensor\n",
        "{decoder_initial_state}\n",
        "\"\"\")\n",
        "\n",
        "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "# One heuristic is to decode up to two times the source sentence lengths.\n",
        "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "#initialize inference decoder\n",
        "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens,\n",
        "                             end_token=end_token,\n",
        "                             initial_state = decoder_initial_state)\n",
        "#print( first_finished.shape)\n",
        "print(f\"first_inputs returns the same decoder_input i.e. embedding of  {SOS} : {first_inputs.shape}\")\n",
        "print(f\"start_index_emb_avg {tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))}\") # mean along the batch\n",
        "\n",
        "inputs = first_inputs\n",
        "state = first_state  \n",
        "predictions = np.empty((inference_batch_size,0), dtype = np.int32)                                                                             \n",
        "for j in range(maximum_iterations):\n",
        "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "    inputs = next_inputs\n",
        "    state = next_state\n",
        "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
        "    predictions = np.append(predictions, outputs, axis = -1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_tx : (1, 1024)\n",
            "c_tx : (1, 1024)\n",
            "decoder_initial_state = [a_tx, c_tx] : (2, 1, 1024)\n",
            "\n",
            "Compared to simple encoder-decoder without attention, the decoder_initial_state\n",
            "is an AttentionWrapperState object containing s_prev tensors and context and alignment vector\n",
            "\n",
            "decoder initial state shape: (6,)\n",
            "decoder_initial_state tensor\n",
            "AttentionWrapperState(cell_state=[<tf.Tensor: shape=(1, 1024), dtype=float32, numpy=\n",
            "array([[-0.02423859, -0.04404252, -0.08185287, ..., -0.00134279,\n",
            "         0.07152605,  0.01424581]], dtype=float32)>, <tf.Tensor: shape=(1, 1024), dtype=float32, numpy=\n",
            "array([[-0.05931591, -0.10267326, -0.21703617, ..., -0.00413214,\n",
            "         0.1776058 ,  0.0562396 ]], dtype=float32)>], attention=<tf.Tensor: shape=(1, 1024), dtype=float32, numpy=array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, time=<tf.Tensor: shape=(), dtype=int32, numpy=0>, alignments=<tf.Tensor: shape=(1, 8), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, alignment_history=(), attention_state=<tf.Tensor: shape=(1, 8), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>)\n",
            "\n",
            "first_inputs returns the same decoder_input i.e. embedding of  <start> : (1, 256)\n",
            "start_index_emb_avg 0.6587639451026917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI0xqsbKeo3t",
        "outputId": "e07c4e74-db5b-4c52-8f40-3268720021aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#prediction based on our sentence earlier\n",
        "print(\"English Sentence:\")\n",
        "print(input_raw)\n",
        "print(\"\\nGerman Translation:\")\n",
        "for i in range(len(predictions)):\n",
        "    line = predictions[i,:]\n",
        "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
        "    print(\" \".join( [ge_tokenizer.index_word[w] for w in seq]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Sentence:\n",
            "how are you\n",
            "\n",
            "German Translation:\n",
            "sie sind wenig\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7EdExVwepqg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}