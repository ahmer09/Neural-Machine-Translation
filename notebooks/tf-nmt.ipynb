{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of tf-nmt.ipynb","provenance":[{"file_id":"14upowIYmQvfoFrUzIZ1nc-4NAmxvDChH","timestamp":1602189485086}],"toc_visible":true,"authorship_tag":"ABX9TyM4ngpJPOdvBO2r0EoS/uI3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"gDhcZb20ek-g","executionInfo":{"status":"ok","timestamp":1602180395437,"user_tz":-120,"elapsed":1608,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}},"outputId":"04baeea5-0930-43f4-c189-d6cd2e911fe5","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["!ls -lrt\n","#download data\n","print(\"Downloading Dataset:\")\n","!wget --quiet http://www.manythings.org/anki/deu-eng.zip\n","!unzip deu-eng.zip"],"execution_count":5,"outputs":[{"output_type":"stream","text":["total 4\n","drwxr-xr-x 1 root root 4096 Oct  5 16:31 sample_data\n","Downloading Dataset:\n","Archive:  deu-eng.zip\n","  inflating: deu.txt                 \n","  inflating: _about.txt              \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jb5Nyl2Jilu5","executionInfo":{"status":"ok","timestamp":1602180020512,"user_tz":-120,"elapsed":427,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}},"outputId":"f4d2663b-7284-405c-811f-558400590b4e","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["ls -lrt"],"execution_count":3,"outputs":[{"output_type":"stream","text":["total 4\n","drwxr-xr-x 1 root root 4096 Oct  5 16:31 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zH5-cYUOergC","executionInfo":{"status":"ok","timestamp":1602166554786,"user_tz":-120,"elapsed":471,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["import csv\n","import string\n","import re\n","from typing import List, Tuple\n","from pickle import dump\n","from unicodedata import normalize\n","import numpy as np\n","import itertools\n","from pickle import load\n","from tensorflow.keras.utils import to_categorical\n","from keras.utils.vis_utils import plot_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Embedding\n","from pickle import load\n","import random\n","import tensorflow as tf\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu\n","from sklearn.model_selection import train_test_split\n","import tensorflow_addons as tfa"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgiBBy92exw8","executionInfo":{"status":"ok","timestamp":1602166556539,"user_tz":-120,"elapsed":648,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["# Start of sentence\n","SOS = \"<start>\"\n","# End of sentence\n","EOS = \"<end>\"\n","# Relavent Punctuation\n","PUNCTUATION = set(\"?,!,.\")"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"wI7DkqnpfIGN","executionInfo":{"status":"ok","timestamp":1602166557707,"user_tz":-120,"elapsed":471,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["def load_dataset(filename: str) -> str:\n","  \"\"\"\n","  load dataset into memory\n","  \"\"\"\n","  with open(filename, mode=\"rt\", encoding=\"utf-8\") as fp:\n","    return fp.read()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqpy0jfwfooF","executionInfo":{"status":"ok","timestamp":1602166558721,"user_tz":-120,"elapsed":333,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["def to_pairs(dataset: str, limit: int=None, shuffle=False) -> List[Tuple[str, str]]:\n","  \"\"\"\n","  Split dataset into pair of sentences\n","  :param dataset: dataset containing examples of translations between two languages\n","  the examples are delimited by `\\n` and the contents of the lines are\n","  delimited by `\\t`\n","  :param limit: number that limit dataset size (optional)\n","  :param shuffle: default is True\n","  :return: list of pairs\n","  \"\"\"\n","  assert isinstance(limit, (int, type(None))), TypeError(\n","      \"The limit value must be an integer\"\n","  )\n","  lines = dataset.strip().split('\\n')\n","  # Random Dataset\n","  if shuffle is True:\n","    random.shuffle(lines)\n","  number_examples = limit or len(lines)\n","  pairs = []\n","  for line in lines[: abs(number_examples)]:\n","    # take only source and target\n","    src, trg, _ = line.split(\"\\t\")\n","    pairs.append((src, trg))\n","\n","  # dataset size check\n","  assert len(pairs) == number_examples\n","  return pairs"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSJ-HZIVji4f","executionInfo":{"status":"ok","timestamp":1602166559968,"user_tz":-120,"elapsed":437,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["def separe_punctuation(token: str) -> str:\n","  \"\"\"\n","  Separe punctuation if they exist\n","  \"\"\"\n","  if not set(token).intersection(PUNCTUATION):\n","    return token\n","  for p in PUNCTUATION:\n","    token = f\" {p} \".join(token.split(p))\n","  return \" \".join(token.split(p))    "],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WC5FstwGKhp","executionInfo":{"status":"ok","timestamp":1602166598016,"user_tz":-120,"elapsed":454,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["def preprocess(sentence: str, add_start_end: bool=True) -> str:\n","  \"\"\"\n","  - convert lowercase\n","  - remove numbers\n","  - remove speacial characters\n","  - separe punctuation\n","  - add <start> and <end> of sentence\n","  :param add_start_end: add SOS and EOS\n","  \"\"\"\n","  re_print = re.compile(f\"[^{re.escape(string.printable)}]\")\n","  # convert lowercase and normalizing unicode characters\n","  sentence = (\n","      normalize(\"NFD\", sentence.lower()).encode(\"ascii\", \"ignore\").decode(\"UTF-8\")\n","  )\n","  cleaned_tokens = []\n","  # tokenize sentence on white space\n","  for token in sentence.split():\n","    # remove non printable characters from each token\n","    token = re_print.sub(\"\", token).strip()\n","    # ignore token with numbers\n","    if re.findall(\"[0-9]\", token):\n","      continue\n","    # add space between words and punctuation\n","    token = separe_punctuation(token)\n","    cleaned_tokens.append(token)\n","\n","  # rebuild sentence with space between tokens\n","  sentence = \" \".join(cleaned_tokens)\n","\n","  # add a start and end token to the sentence\n","  if add_start_end is True:\n","    sentence = f\"{SOS} {sentence} {EOS}\"\n","  return sentence      "],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVyQLrfNR7iC","executionInfo":{"status":"ok","timestamp":1602166562405,"user_tz":-120,"elapsed":534,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["def dataset_preprocess(dataset: List[Tuple[str, str]]) -> Tuple[List[str], List[str]]:\n","  \"\"\"\n","  Returns processed dataset\n","\n","  :param dataset: list of sentence pairs\n","  :return: list of prallel data\n","  \"\"\"\n","  source_cleaned = []\n","  target_cleaned = []\n","  for source, target in dataset:\n","    source_cleaned.append(preprocess(source))\n","    target_cleaned.append(preprocess(target))\n","  return source_cleaned, target_cleaned  \n"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwKm5lYTiHMH","executionInfo":{"status":"ok","timestamp":1602166602244,"user_tz":-120,"elapsed":1053,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}},"outputId":"244e2b50-ab97-4838-c44d-90c27a83ea73","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["NUM_EXAMPLES = 10000 # Limit dataset size\n","filename = \"deu.txt\"\n","dataset = load_dataset(filename)\n","# get pairs limited to 1000 pairs\n","pairs = to_pairs(dataset, limit=NUM_EXAMPLES)\n","print(f\"Dataset size: {len(pairs)}\")\n","raw_data_en, raw_data_ge = dataset_preprocess(pairs)\n","\n","# show last 5 pairs\n","for pair in zip(raw_data_en[-5:],raw_data_ge[-5:]):\n","    print(pair)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Dataset size: 10000\n","('<start> tom was crying .  <end>', '<start> tom flennte .  <end>')\n","('<start> tom was eating .  <end>', '<start> tom hat gegessen .  <end>')\n","('<start> tom was famous .  <end>', '<start> tom war beruhmt .  <end>')\n","('<start> tom was framed .  <end>', '<start> tom wurde reingelegt .  <end>')\n","('<start> tom was fuming .  <end>', '<start> tom war wutend .  <end>')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pafn9rsGkTyK","executionInfo":{"status":"ok","timestamp":1602167843766,"user_tz":-120,"elapsed":794,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","en_tokenizer.fit_on_texts(raw_data_en)\n","\n","data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n","data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')\n","\n","ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","ge_tokenizer.fit_on_texts(raw_data_ge)\n","\n","data_ge = ge_tokenizer.texts_to_sequences(raw_data_ge)\n","data_ge = tf.keras.preprocessing.sequence.pad_sequences(data_ge, padding='post')"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQPKXRUopSgD"},"source":["def max_len(tensor):\n","  # print ()"],"execution_count":null,"outputs":[]}]}