{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of tf-nmt.ipynb","provenance":[{"file_id":"14upowIYmQvfoFrUzIZ1nc-4NAmxvDChH","timestamp":1602189485086}],"toc_visible":true,"authorship_tag":"ABX9TyM3xY637VBkz6INK8HXwXYM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"gDhcZb20ek-g","executionInfo":{"status":"ok","timestamp":1602343940661,"user_tz":-120,"elapsed":2043,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}},"outputId":"bf060ca0-e06b-4d0f-ec30-db02b904b990","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["  !ls -lrt\n","#download data\n","print(\"Downloading Dataset:\")\n","!wget --quiet http://www.manythings.org/anki/deu-eng.zip\n","!unzip deu-eng.zip"],"execution_count":1,"outputs":[{"output_type":"stream","text":["total 4\n","drwxr-xr-x 1 root root 4096 Oct  5 16:31 sample_data\n","Downloading Dataset:\n","Archive:  deu-eng.zip\n","  inflating: deu.txt                 \n","  inflating: _about.txt              \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jb5Nyl2Jilu5","executionInfo":{"status":"ok","timestamp":1602343941493,"user_tz":-120,"elapsed":2867,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}},"outputId":"cdf2f055-0c1e-49fa-8875-f2292f16929c","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["ls -lrt"],"execution_count":2,"outputs":[{"output_type":"stream","text":["total 41828\n","-rw-r--r-- 1 root root  8324970 Aug 23 14:57 deu-eng.zip\n","-rw-r--r-- 1 root root 34494242 Aug 23 23:57 deu.txt\n","-rw-r--r-- 1 root root     1441 Aug 23 23:57 _about.txt\n","drwxr-xr-x 1 root root     4096 Oct  5 16:31 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zH5-cYUOergC","executionInfo":{"status":"ok","timestamp":1602343943784,"user_tz":-120,"elapsed":5156,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["import csv\n","import string\n","import re\n","from typing import List, Tuple\n","from pickle import dump\n","from unicodedata import normalize\n","import numpy as np\n","import itertools\n","from pickle import load\n","from tensorflow.keras.utils import to_categorical\n","from keras.utils.vis_utils import plot_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Embedding\n","from pickle import load\n","import random\n","import tensorflow as tf\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu\n","from sklearn.model_selection import train_test_split\n","import tensorflow_addons as tfa"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgiBBy92exw8","executionInfo":{"status":"ok","timestamp":1602343943793,"user_tz":-120,"elapsed":5162,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["# Start of sentence\n","SOS = \"<start>\"\n","# End of sentence\n","EOS = \"<end>\"\n","# Relavent Punctuation\n","PUNCTUATION = set(\"?,!,.\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"wI7DkqnpfIGN","executionInfo":{"status":"ok","timestamp":1602343943795,"user_tz":-120,"elapsed":5162,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["def load_dataset(filename: str) -> str:\n","  \"\"\"\n","  load dataset into memory\n","  \"\"\"\n","  with open(filename, mode=\"rt\", encoding=\"utf-8\") as fp:\n","    return fp.read()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqpy0jfwfooF","executionInfo":{"status":"ok","timestamp":1602343943796,"user_tz":-120,"elapsed":5161,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["def to_pairs(dataset: str, limit: int=None, shuffle=False) -> List[Tuple[str, str]]:\n","  \"\"\"\n","  Split dataset into pair of sentences\n","  :param dataset: dataset containing examples of translations between two languages\n","  the examples are delimited by `\\n` and the contents of the lines are\n","  delimited by `\\t`\n","  :param limit: number that limit dataset size (optional)\n","  :param shuffle: default is True\n","  :return: list of pairs\n","  \"\"\"\n","  assert isinstance(limit, (int, type(None))), TypeError(\n","      \"The limit value must be an integer\"\n","  )\n","  lines = dataset.strip().split('\\n')\n","  # Random Dataset\n","  if shuffle is True:\n","    random.shuffle(lines)\n","  number_examples = limit or len(lines)\n","  pairs = []\n","  for line in lines[: abs(number_examples)]:\n","    # take only source and target\n","    src, trg, _ = line.split(\"\\t\")\n","    pairs.append((src, trg))\n","\n","  # dataset size check\n","  assert len(pairs) == number_examples\n","  return pairs"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSJ-HZIVji4f","executionInfo":{"status":"ok","timestamp":1602343943798,"user_tz":-120,"elapsed":5161,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["def separe_punctuation(token: str) -> str:\n","  \"\"\"\n","  Separe punctuation if they exist\n","  \"\"\"\n","  if not set(token).intersection(PUNCTUATION):\n","    return token\n","  for p in PUNCTUATION:\n","    token = f\" {p} \".join(token.split(p))\n","  return \" \".join(token.split(p))    "],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WC5FstwGKhp","executionInfo":{"status":"ok","timestamp":1602343943799,"user_tz":-120,"elapsed":5160,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["def preprocess(sentence: str, add_start_end: bool=True) -> str:\n","  \"\"\"\n","  - convert lowercase\n","  - remove numbers\n","  - remove speacial characters\n","  - separe punctuation\n","  - add <start> and <end> of sentence\n","  :param add_start_end: add SOS and EOS\n","  \"\"\"\n","  re_print = re.compile(f\"[^{re.escape(string.printable)}]\")\n","  # convert lowercase and normalizing unicode characters\n","  sentence = (\n","      normalize(\"NFD\", sentence.lower()).encode(\"ascii\", \"ignore\").decode(\"UTF-8\")\n","  )\n","  cleaned_tokens = []\n","  # tokenize sentence on white space\n","  for token in sentence.split():\n","    # remove non printable characters from each token\n","    token = re_print.sub(\"\", token).strip()\n","    # ignore token with numbers\n","    if re.findall(\"[0-9]\", token):\n","      continue\n","    # add space between words and punctuation\n","    token = separe_punctuation(token)\n","    cleaned_tokens.append(token)\n","\n","  # rebuild sentence with space between tokens\n","  sentence = \" \".join(cleaned_tokens)\n","\n","  # add a start and end token to the sentence\n","  if add_start_end is True:\n","    sentence = f\"{SOS} {sentence} {EOS}\"\n","  return sentence      "],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVyQLrfNR7iC","executionInfo":{"status":"ok","timestamp":1602343943800,"user_tz":-120,"elapsed":5159,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["def dataset_preprocess(dataset: List[Tuple[str, str]]) -> Tuple[List[str], List[str]]:\n","  \"\"\"\n","  Returns processed dataset\n","\n","  :param dataset: list of sentence pairs\n","  :return: list of prallel data\n","  \"\"\"\n","  source_cleaned = []\n","  target_cleaned = []\n","  for source, target in dataset:\n","    source_cleaned.append(preprocess(source))\n","    target_cleaned.append(preprocess(target))\n","  return source_cleaned, target_cleaned  \n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwKm5lYTiHMH","executionInfo":{"status":"ok","timestamp":1602343944550,"user_tz":-120,"elapsed":5902,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}},"outputId":"a485f203-60bc-4022-d896-07e968a5dc71","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["NUM_EXAMPLES = 10000 # Limit dataset size\n","filename = \"deu.txt\"\n","dataset = load_dataset(filename)\n","# get pairs limited to 1000 pairs\n","pairs = to_pairs(dataset, limit=NUM_EXAMPLES)\n","print(f\"Dataset size: {len(pairs)}\")\n","raw_data_en, raw_data_ge = dataset_preprocess(pairs)\n","\n","# show last 5 pairs\n","for pair in zip(raw_data_en[-5:],raw_data_ge[-5:]):\n","    print(pair)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Dataset size: 10000\n","('<start> tom was crying    <end>', '<start> tom flennte    <end>')\n","('<start> tom was eating    <end>', '<start> tom hat gegessen    <end>')\n","('<start> tom was famous    <end>', '<start> tom war beruhmt    <end>')\n","('<start> tom was framed    <end>', '<start> tom wurde reingelegt    <end>')\n","('<start> tom was fuming    <end>', '<start> tom war wutend    <end>')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pafn9rsGkTyK","executionInfo":{"status":"ok","timestamp":1602343945253,"user_tz":-120,"elapsed":6597,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","en_tokenizer.fit_on_texts(raw_data_en)\n","\n","data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n","data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')\n","\n","ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","ge_tokenizer.fit_on_texts(raw_data_ge)\n","\n","data_ge = ge_tokenizer.texts_to_sequences(raw_data_ge)\n","data_ge = tf.keras.preprocessing.sequence.pad_sequences(data_ge, padding='post')"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQPKXRUopSgD","executionInfo":{"status":"ok","timestamp":1602343945254,"user_tz":-120,"elapsed":6585,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["def max_len(tensor):\n","    #print( np.argmax([len(t) for t in tensor]))\n","    return max( len(t) for t in tensor)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfBasZugI9LF","executionInfo":{"status":"ok","timestamp":1602343945254,"user_tz":-120,"elapsed":6581,"user":{"displayName":"hammer ai","photoUrl":"","userId":"05738756673398875152"}}},"source":["X_train, X_test, Y_train, Y_test = train_test_split(data_en, data_ge, test_size=0.2)\n","BATCH_SIZE = 64\n","BUFFER_SIZE = len(X_train)\n","steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n","embedding_dims = 256\n","rnn_units = 1024\n","dense_units = 1024\n","Dtype = tf."],"execution_count":12,"outputs":[]}]}